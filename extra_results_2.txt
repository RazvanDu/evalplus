Running big command HF, run=1
Greedy decoding ON (--greedy): setting bs=1, n_samples=1, temperature=0
Initializing a decoder model: meta-llama/Llama-3.1-70B-Instruct ...
kwargs = {'batch_size': 1, 'temperature': 0.0, 'instruction_prefix': 'Please provide a self-contained Python script that solves the following problem in a markdown code block:', 'response_prefix': 'Below is a Python script with a self-contained function that solves the problem and passes corresponding tests:', 'trust_remote_code': 'true', 'dtype': 'bfloat16'}
self.eos = ['<|endoftext|>', '<|endofmask|>', '</s>', '\nif __name__', '\ndef main(', '\nprint(', '\n```\n']
/opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c104cuda9SetDeviceEi'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Loading checkpoint shards:   0%|                                                                                                                                                                      | 0/30 [00:00<?, ?it/s]Loading checkpoint shards:   3%|█████▎                                                                                                                                                        | 1/30 [00:02<01:25,  2.95s/it]Loading checkpoint shards:   7%|██████████▌                                                                                                                                                   | 2/30 [00:06<01:27,  3.12s/it]Loading checkpoint shards:  10%|███████████████▊                                                                                                                                              | 3/30 [00:09<01:26,  3.20s/it]Loading checkpoint shards:  13%|█████████████████████                                                                                                                                         | 4/30 [00:13<01:29,  3.44s/it]Loading checkpoint shards:  17%|██████████████████████████▎                                                                                                                                   | 5/30 [00:17<01:34,  3.77s/it]Loading checkpoint shards:  20%|███████████████████████████████▌                                                                                                                              | 6/30 [00:22<01:37,  4.04s/it]Loading checkpoint shards:  23%|████████████████████████████████████▊                                                                                                                         | 7/30 [00:25<01:30,  3.92s/it]Loading checkpoint shards:  27%|██████████████████████████████████████████▏                                                                                                                   | 8/30 [00:29<01:26,  3.92s/it]Loading checkpoint shards:  30%|███████████████████████████████████████████████▍                                                                                                              | 9/30 [00:33<01:23,  3.97s/it]Loading checkpoint shards:  33%|████████████████████████████████████████████████████▎                                                                                                        | 10/30 [00:37<01:16,  3.83s/it]Loading checkpoint shards:  37%|█████████████████████████████████████████████████████████▌                                                                                                   | 11/30 [00:41<01:12,  3.82s/it]Loading checkpoint shards:  40%|██████████████████████████████████████████████████████████████▊                                                                                              | 12/30 [00:44<01:08,  3.80s/it]Loading checkpoint shards:  43%|████████████████████████████████████████████████████████████████████                                                                                         | 13/30 [00:49<01:07,  3.94s/it]Loading checkpoint shards:  47%|█████████████████████████████████████████████████████████████████████████▎                                                                                   | 14/30 [00:54<01:09,  4.37s/it]Loading checkpoint shards:  50%|██████████████████████████████████████████████████████████████████████████████▌                                                                              | 15/30 [00:58<01:02,  4.20s/it]Loading checkpoint shards:  53%|███████████████████████████████████████████████████████████████████████████████████▋                                                                         | 16/30 [01:01<00:55,  4.00s/it]Loading checkpoint shards:  57%|████████████████████████████████████████████████████████████████████████████████████████▉                                                                    | 17/30 [01:05<00:50,  3.88s/it]Loading checkpoint shards:  60%|██████████████████████████████████████████████████████████████████████████████████████████████▏                                                              | 18/30 [01:09<00:48,  4.02s/it]Loading checkpoint shards:  63%|███████████████████████████████████████████████████████████████████████████████████████████████████▍                                                         | 19/30 [01:13<00:43,  3.98s/it]Loading checkpoint shards:  67%|████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                    | 20/30 [01:17<00:39,  3.96s/it]Loading checkpoint shards:  70%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                               | 21/30 [01:21<00:34,  3.86s/it]Loading checkpoint shards:  73%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                         | 22/30 [01:25<00:30,  3.83s/it]Loading checkpoint shards:  77%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                    | 23/30 [01:28<00:26,  3.80s/it]Loading checkpoint shards:  80%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                               | 24/30 [01:32<00:22,  3.74s/it]Loading checkpoint shards:  83%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                          | 25/30 [01:35<00:18,  3.63s/it]Loading checkpoint shards:  87%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                     | 26/30 [01:39<00:14,  3.64s/it]Loading checkpoint shards:  90%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎               | 27/30 [01:42<00:10,  3.53s/it]Loading checkpoint shards:  93%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌          | 28/30 [01:46<00:07,  3.67s/it]Loading checkpoint shards:  97%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊     | 29/30 [01:50<00:03,  3.63s/it]Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [01:52<00:00,  3.29s/it]Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [01:52<00:00,  3.76s/it]
Sanitized code outputs will be saved to evalplus_results/humaneval/meta-llama--Llama-3.1-70B-Instruct_hf_temp_0.0_gamma_5_134640.jsonl
Raw outputs will be saved to evalplus_results/humaneval/meta-llama--Llama-3.1-70B-Instruct_hf_temp_0.0_gamma_5_134640.raw.jsonl
Codegen: HumanEval/0 @ meta-llama/Llama-3.1-70B-Instruct
/tmp/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/tmp/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
627
430
Total tokens generated so far 430
Codegen: HumanEval/1 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
615
427
Total tokens generated so far 857
Codegen: HumanEval/2 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
510
352
Total tokens generated so far 1209
Codegen: HumanEval/3 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
546
354
Total tokens generated so far 1563
Codegen: HumanEval/4 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
533
343
Total tokens generated so far 1906
Codegen: HumanEval/5 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
548
383
Total tokens generated so far 2289
Codegen: HumanEval/6 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
542
357
Total tokens generated so far 2646
Codegen: HumanEval/7 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
475
309
Total tokens generated so far 2955
Codegen: HumanEval/8 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
566
383
Total tokens generated so far 3338
Codegen: HumanEval/9 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
619
446
Total tokens generated so far 3784
Codegen: HumanEval/10 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
588
373
Total tokens generated so far 4157
Codegen: HumanEval/11 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
519
370
Total tokens generated so far 4527
Codegen: HumanEval/12 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
390
215
Total tokens generated so far 4742
Codegen: HumanEval/13 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
440
297
Total tokens generated so far 5039
Codegen: HumanEval/14 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
347
215
Total tokens generated so far 5254
Codegen: HumanEval/15 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
356
215
Total tokens generated so far 5469
Codegen: HumanEval/16 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
404
261
Total tokens generated so far 5730
Codegen: HumanEval/17 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
699
459
Total tokens generated so far 6189
Codegen: HumanEval/18 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
560
399
Total tokens generated so far 6588
Codegen: HumanEval/19 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
586
408
Total tokens generated so far 6996
Codegen: HumanEval/20 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
674
435
Total tokens generated so far 7431
Codegen: HumanEval/21 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
762
566
Total tokens generated so far 7997
Codegen: HumanEval/22 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
407
243
Total tokens generated so far 8240
Codegen: HumanEval/23 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
282
166
Total tokens generated so far 8406
Codegen: HumanEval/24 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
548
423
Total tokens generated so far 8829
Codegen: HumanEval/25 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
581
382
Total tokens generated so far 9211
Codegen: HumanEval/26 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
511
354
Total tokens generated so far 9565
Codegen: HumanEval/27 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
316
195
Total tokens generated so far 9760
Codegen: HumanEval/28 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
460
330
Total tokens generated so far 10090
Codegen: HumanEval/29 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
451
291
Total tokens generated so far 10381
Codegen: HumanEval/30 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
416
230
Total tokens generated so far 10611
Codegen: HumanEval/31 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
490
318
Total tokens generated so far 10929
Codegen: HumanEval/32 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
765
433
Total tokens generated so far 11362
Codegen: HumanEval/33 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
646
423
Total tokens generated so far 11785
Codegen: HumanEval/34 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
457
311
Total tokens generated so far 12096
Codegen: HumanEval/35 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
463
308
Total tokens generated so far 12404
Codegen: HumanEval/36 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
439
287
Total tokens generated so far 12691
Codegen: HumanEval/37 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
541
345
Total tokens generated so far 13036
Codegen: HumanEval/38 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
1255
1024
Total tokens generated so far 14060
Codegen: HumanEval/39 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
537
367
Total tokens generated so far 14427
Codegen: HumanEval/40 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
686
454
Total tokens generated so far 14881
Codegen: HumanEval/41 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
453
229
Total tokens generated so far 15110
Codegen: HumanEval/42 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
495
314
Total tokens generated so far 15424
Codegen: HumanEval/43 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
498
268
Total tokens generated so far 15692
Codegen: HumanEval/44 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
381
217
Total tokens generated so far 15909
Codegen: HumanEval/45 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
331
212
Total tokens generated so far 16121
Codegen: HumanEval/46 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
673
429
Total tokens generated so far 16550
Codegen: HumanEval/47 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
569
421
Total tokens generated so far 16971
Codegen: HumanEval/48 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
391
247
Total tokens generated so far 17218
Codegen: HumanEval/49 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
544
366
Total tokens generated so far 17584
Codegen: HumanEval/50 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
564
401
Total tokens generated so far 17985
Codegen: HumanEval/51 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
474
279
Total tokens generated so far 18264
Codegen: HumanEval/52 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
393
241
Total tokens generated so far 18505
Codegen: HumanEval/53 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
398
272
Total tokens generated so far 18777
Codegen: HumanEval/54 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
520
307
Total tokens generated so far 19084
Codegen: HumanEval/55 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
441
316
Total tokens generated so far 19400
Codegen: HumanEval/56 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
478
313
Total tokens generated so far 19713
Codegen: HumanEval/57 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
508
348
Total tokens generated so far 20061
Codegen: HumanEval/58 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
579
392
Total tokens generated so far 20453
Codegen: HumanEval/59 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
810
674
Total tokens generated so far 21127
Codegen: HumanEval/60 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
395
228
Total tokens generated so far 21355
Codegen: HumanEval/61 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
470
308
Total tokens generated so far 21663
Codegen: HumanEval/62 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
493
312
Total tokens generated so far 21975
Codegen: HumanEval/63 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
641
419
Total tokens generated so far 22394
Codegen: HumanEval/64 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
678
479
Total tokens generated so far 22873
Codegen: HumanEval/65 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
522
367
Total tokens generated so far 23240
Codegen: HumanEval/66 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
398
213
Total tokens generated so far 23453
Codegen: HumanEval/67 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
649
366
Total tokens generated so far 23819
Codegen: HumanEval/68 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
818
433
Total tokens generated so far 24252
Codegen: HumanEval/69 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
701
463
Total tokens generated so far 24715
Codegen: HumanEval/70 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
584
387
Total tokens generated so far 25102
Codegen: HumanEval/71 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
596
402
Total tokens generated so far 25504
Codegen: HumanEval/72 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
685
382
Total tokens generated so far 25886
Codegen: HumanEval/73 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
605
375
Total tokens generated so far 26261
Codegen: HumanEval/74 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
700
434
Total tokens generated so far 26695
Codegen: HumanEval/75 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
860
708
Total tokens generated so far 27403
Codegen: HumanEval/76 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
537
322
Total tokens generated so far 27725
Codegen: HumanEval/77 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
427
244
Total tokens generated so far 27969
Codegen: HumanEval/78 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
820
448
Total tokens generated so far 28417
Codegen: HumanEval/79 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
428
216
Total tokens generated so far 28633
Codegen: HumanEval/80 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
564
370
Total tokens generated so far 29003
Codegen: HumanEval/81 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
842
499
Total tokens generated so far 29502
Codegen: HumanEval/82 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
368
214
Total tokens generated so far 29716
Codegen: HumanEval/83 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
480
363
Total tokens generated so far 30079
Codegen: HumanEval/84 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
499
293
Total tokens generated so far 30372
Codegen: HumanEval/85 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
419
291
Total tokens generated so far 30663
Codegen: HumanEval/86 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
422
219
Total tokens generated so far 30882
Codegen: HumanEval/87 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
858
526
Total tokens generated so far 31408
Codegen: HumanEval/88 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
668
392
Total tokens generated so far 31800
Codegen: HumanEval/89 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
541
358
Total tokens generated so far 32158
Codegen: HumanEval/90 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
484
287
Total tokens generated so far 32445
Codegen: HumanEval/91 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
499
319
Total tokens generated so far 32764
Codegen: HumanEval/92 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
545
337
Total tokens generated so far 33101
Codegen: HumanEval/93 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
548
366
Total tokens generated so far 33467
Codegen: HumanEval/94 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
878
519
Total tokens generated so far 33986
Codegen: HumanEval/95 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
671
429
Total tokens generated so far 34415
Codegen: HumanEval/96 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
566
348
Total tokens generated so far 34763
Codegen: HumanEval/97 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
450
279
Total tokens generated so far 35042
Codegen: HumanEval/98 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
426
281
Total tokens generated so far 35323
Codegen: HumanEval/99 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
604
375
Total tokens generated so far 35698
Codegen: HumanEval/100 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
625
420
Total tokens generated so far 36118
Codegen: HumanEval/101 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
482
293
Total tokens generated so far 36411
Codegen: HumanEval/102 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
858
691
Total tokens generated so far 37102
Codegen: HumanEval/103 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
573
360
Total tokens generated so far 37462
Codegen: HumanEval/104 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
450
275
Total tokens generated so far 37737
Codegen: HumanEval/105 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
775
426
Total tokens generated so far 38163
Codegen: HumanEval/106 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
585
388
Total tokens generated so far 38551
Codegen: HumanEval/107 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
640
356
Total tokens generated so far 38907
Codegen: HumanEval/108 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
535
338
Total tokens generated so far 39245
Codegen: HumanEval/109 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
691
319
Total tokens generated so far 39564
Codegen: HumanEval/110 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
768
498
Total tokens generated so far 40062
Codegen: HumanEval/111 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
762
538
Total tokens generated so far 40600
Codegen: HumanEval/112 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
574
339
Total tokens generated so far 40939
Codegen: HumanEval/113 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
666
420
Total tokens generated so far 41359
Codegen: HumanEval/114 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
486
325
Total tokens generated so far 41684
Codegen: HumanEval/115 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
738
343
Total tokens generated so far 42027
Codegen: HumanEval/116 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
551
308
Total tokens generated so far 42335
Codegen: HumanEval/117 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
723
469
Total tokens generated so far 42804
Codegen: HumanEval/118 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
562
340
Total tokens generated so far 43144
Codegen: HumanEval/119 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
573
332
Total tokens generated so far 43476
Codegen: HumanEval/120 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
623
333
Total tokens generated so far 43809
Codegen: HumanEval/121 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
481
311
Total tokens generated so far 44120
Codegen: HumanEval/122 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
608
407
Total tokens generated so far 44527
Codegen: HumanEval/123 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
1328
1024
Total tokens generated so far 45551
Codegen: HumanEval/124 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
1017
681
Total tokens generated so far 46232
Codegen: HumanEval/125 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
601
388
Total tokens generated so far 46620
Codegen: HumanEval/126 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
779
457
Total tokens generated so far 47077
Codegen: HumanEval/127 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
727
400
Total tokens generated so far 47477
Codegen: HumanEval/128 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
492
297
Total tokens generated so far 47774
Codegen: HumanEval/129 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
1251
782
Total tokens generated so far 48556
Codegen: HumanEval/130 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
898
590
Total tokens generated so far 49146
Codegen: HumanEval/131 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
441
297
Total tokens generated so far 49443
Codegen: HumanEval/132 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
527
321
Total tokens generated so far 49764
Codegen: HumanEval/133 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
483
253
Total tokens generated so far 50017
Codegen: HumanEval/134 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
531
321
Total tokens generated so far 50338
Codegen: HumanEval/135 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
485
309
Total tokens generated so far 50647
Codegen: HumanEval/136 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
640
429
Total tokens generated so far 51076
Codegen: HumanEval/137 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
623
409
Total tokens generated so far 51485
Codegen: HumanEval/138 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
560
410
Total tokens generated so far 51895
Codegen: HumanEval/139 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
527
359
Total tokens generated so far 52254
Codegen: HumanEval/140 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
632
453
Total tokens generated so far 52707
Codegen: HumanEval/141 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
904
617
Total tokens generated so far 53324
Codegen: HumanEval/142 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
543
301
Total tokens generated so far 53625
Codegen: HumanEval/143 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
506
286
Total tokens generated so far 53911
Codegen: HumanEval/144 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
585
357
Total tokens generated so far 54268
Codegen: HumanEval/145 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
620
432
Total tokens generated so far 54700
Codegen: HumanEval/146 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
600
408
Total tokens generated so far 55108
Codegen: HumanEval/147 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
580
342
Total tokens generated so far 55450
Codegen: HumanEval/148 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
711
427
Total tokens generated so far 55877
Codegen: HumanEval/149 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
568
308
Total tokens generated so far 56185
Codegen: HumanEval/150 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
506
347
Total tokens generated so far 56532
Codegen: HumanEval/151 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
479
269
Total tokens generated so far 56801
Codegen: HumanEval/152 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
630
327
Total tokens generated so far 57128
Codegen: HumanEval/153 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
752
402
Total tokens generated so far 57530
Codegen: HumanEval/154 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
598
396
Total tokens generated so far 57926
Codegen: HumanEval/155 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
527
389
Total tokens generated so far 58315
Codegen: HumanEval/156 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
587
413
Total tokens generated so far 58728
Codegen: HumanEval/157 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
527
346
Total tokens generated so far 59074
Codegen: HumanEval/158 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
573
380
Total tokens generated so far 59454
Codegen: HumanEval/159 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
760
410
Total tokens generated so far 59864
Codegen: HumanEval/160 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
827
547
Total tokens generated so far 60411
Codegen: HumanEval/161 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
517
337
Total tokens generated so far 60748
Codegen: HumanEval/162 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
406
257
Total tokens generated so far 61005
Codegen: HumanEval/163 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
518
341
Total tokens generated so far 61346
HuggingFaceDecoder •100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 164/164 • 3:22:08
Execution time exactly: 12128.42 seconds
Computing expected output...
Expected outputs computed in 16.79s
Reading samples...
0it [00:00, ?it/s]1it [00:00,  1.91it/s]164it [00:00, 307.97it/s]
  0%|                                                                                                                                                                                                | 0/164 [00:00<?, ?it/s]  1%|█                                                                                                                                                                                       | 1/164 [00:00<01:26,  1.89it/s]  5%|████████▉                                                                                                                                                                               | 8/164 [00:00<00:11, 14.04it/s]  7%|████████████▎                                                                                                                                                                          | 11/164 [00:01<00:13, 11.12it/s] 10%|█████████████████▊                                                                                                                                                                     | 16/164 [00:01<00:13, 11.18it/s] 13%|███████████████████████▍                                                                                                                                                               | 21/164 [00:01<00:08, 16.08it/s] 15%|██████████████████████████▊                                                                                                                                                            | 24/164 [00:02<00:11, 12.29it/s] 17%|███████████████████████████████▏                                                                                                                                                       | 28/164 [00:02<00:08, 15.29it/s] 19%|██████████████████████████████████▌                                                                                                                                                    | 31/164 [00:02<00:11, 11.63it/s] 21%|███████████████████████████████████████                                                                                                                                                | 35/164 [00:02<00:09, 13.04it/s] 23%|█████████████████████████████████████████▎                                                                                                                                             | 37/164 [00:03<00:11, 11.09it/s] 24%|███████████████████████████████████████████▌                                                                                                                                           | 39/164 [00:03<00:10, 12.00it/s] 25%|█████████████████████████████████████████████▊                                                                                                                                         | 41/164 [00:03<00:09, 12.31it/s] 26%|███████████████████████████████████████████████▉                                                                                                                                       | 43/164 [00:03<00:10, 11.08it/s] 28%|███████████████████████████████████████████████████▎                                                                                                                                   | 46/164 [00:03<00:08, 13.84it/s] 29%|█████████████████████████████████████████████████████▌                                                                                                                                 | 48/164 [00:03<00:08, 13.07it/s] 30%|███████████████████████████████████████████████████████▊                                                                                                                               | 50/164 [00:04<00:10, 11.31it/s] 34%|██████████████████████████████████████████████████████████████▍                                                                                                                        | 56/164 [00:04<00:07, 15.23it/s] 35%|████████████████████████████████████████████████████████████████▋                                                                                                                      | 58/164 [00:04<00:08, 12.56it/s] 37%|████████████████████████████████████████████████████████████████████                                                                                                                   | 61/164 [00:04<00:07, 14.18it/s] 38%|██████████████████████████████████████████████████████████████████████▎                                                                                                                | 63/164 [00:04<00:06, 14.94it/s] 40%|████████████████████████████████████████████████████████████████████████▌                                                                                                              | 65/164 [00:05<00:08, 11.70it/s] 42%|████████████████████████████████████████████████████████████████████████████▉                                                                                                          | 69/164 [00:05<00:06, 13.97it/s] 44%|████████████████████████████████████████████████████████████████████████████████▎                                                                                                      | 72/164 [00:05<00:06, 14.21it/s] 45%|██████████████████████████████████████████████████████████████████████████████████▌                                                                                                    | 74/164 [00:05<00:06, 14.44it/s] 46%|████████████████████████████████████████████████████████████████████████████████████▊                                                                                                  | 76/164 [00:05<00:05, 14.90it/s] 48%|███████████████████████████████████████████████████████████████████████████████████████                                                                                                | 78/164 [00:06<00:05, 14.79it/s] 49%|█████████████████████████████████████████████████████████████████████████████████████████▎                                                                                             | 80/164 [00:06<00:05, 14.20it/s] 50%|███████████████████████████████████████████████████████████████████████████████████████████▌                                                                                           | 82/164 [00:06<00:06, 12.58it/s] 52%|██████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                        | 85/164 [00:06<00:05, 14.48it/s] 53%|█████████████████████████████████████████████████████████████████████████████████████████████████                                                                                      | 87/164 [00:06<00:05, 14.57it/s] 54%|███████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                                   | 89/164 [00:06<00:06, 11.10it/s] 57%|████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                              | 94/164 [00:07<00:04, 17.32it/s] 59%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                          | 97/164 [00:07<00:05, 12.37it/s] 61%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                       | 100/164 [00:07<00:04, 13.56it/s] 62%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                    | 102/164 [00:08<00:05, 10.52it/s] 63%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                  | 104/164 [00:08<00:05, 11.27it/s] 65%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                | 106/164 [00:08<00:05, 10.52it/s] 66%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                              | 108/164 [00:08<00:05,  9.96it/s] 68%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                         | 112/164 [00:08<00:04, 10.97it/s] 70%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                       | 114/164 [00:09<00:04, 10.46it/s] 73%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                  | 119/164 [00:09<00:03, 12.71it/s] 74%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                               | 121/164 [00:09<00:03, 11.08it/s] 75%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                             | 123/164 [00:10<00:05,  8.09it/s] 80%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                    | 131/164 [00:10<00:03,  9.57it/s] 84%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                            | 138/164 [00:11<00:02, 12.02it/s] 85%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                          | 140/164 [00:11<00:02, 11.84it/s] 88%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                      | 144/164 [00:11<00:01, 14.57it/s] 89%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                    | 146/164 [00:11<00:01, 11.04it/s] 92%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌              | 151/164 [00:12<00:01, 10.14it/s] 95%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████          | 155/164 [00:12<00:00, 13.08it/s] 96%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎      | 158/164 [00:13<00:00, 10.35it/s] 99%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉ | 163/164 [00:14<00:00,  7.37it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 164/164 [00:15<00:00, 10.74it/s]
humaneval (base tests)
pass@1:	0.774
humaneval+ (base + extra tests)
pass@1:	0.713
Execution time for HF, run=1: 12294s
Running big command HF, run=2
Greedy decoding ON (--greedy): setting bs=1, n_samples=1, temperature=0
Initializing a decoder model: meta-llama/Llama-3.1-70B-Instruct ...
kwargs = {'batch_size': 1, 'temperature': 0.0, 'instruction_prefix': 'Please provide a self-contained Python script that solves the following problem in a markdown code block:', 'response_prefix': 'Below is a Python script with a self-contained function that solves the problem and passes corresponding tests:', 'trust_remote_code': 'true', 'dtype': 'bfloat16'}
self.eos = ['<|endoftext|>', '<|endofmask|>', '</s>', '\nif __name__', '\ndef main(', '\nprint(', '\n```\n']
/opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c104cuda9SetDeviceEi'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Loading checkpoint shards:   0%|                                                                                                                                                                      | 0/30 [00:00<?, ?it/s]Loading checkpoint shards:   3%|█████▎                                                                                                                                                        | 1/30 [00:03<01:52,  3.90s/it]Loading checkpoint shards:   7%|██████████▌                                                                                                                                                   | 2/30 [00:07<01:39,  3.54s/it]Loading checkpoint shards:  10%|███████████████▊                                                                                                                                              | 3/30 [00:11<01:41,  3.76s/it]Loading checkpoint shards:  13%|█████████████████████                                                                                                                                         | 4/30 [00:14<01:36,  3.72s/it]Loading checkpoint shards:  17%|██████████████████████████▎                                                                                                                                   | 5/30 [00:18<01:32,  3.71s/it]Loading checkpoint shards:  20%|███████████████████████████████▌                                                                                                                              | 6/30 [00:21<01:23,  3.50s/it]Loading checkpoint shards:  23%|████████████████████████████████████▊                                                                                                                         | 7/30 [00:24<01:17,  3.39s/it]Loading checkpoint shards:  27%|██████████████████████████████████████████▏                                                                                                                   | 8/30 [00:28<01:14,  3.38s/it]Loading checkpoint shards:  30%|███████████████████████████████████████████████▍                                                                                                              | 9/30 [00:31<01:11,  3.39s/it]Loading checkpoint shards:  33%|████████████████████████████████████████████████████▎                                                                                                        | 10/30 [00:35<01:09,  3.48s/it]Loading checkpoint shards:  37%|█████████████████████████████████████████████████████████▌                                                                                                   | 11/30 [00:38<01:04,  3.41s/it]Loading checkpoint shards:  40%|██████████████████████████████████████████████████████████████▊                                                                                              | 12/30 [00:42<01:04,  3.58s/it]Loading checkpoint shards:  43%|████████████████████████████████████████████████████████████████████                                                                                         | 13/30 [00:46<01:01,  3.59s/it]Loading checkpoint shards:  47%|█████████████████████████████████████████████████████████████████████████▎                                                                                   | 14/30 [00:49<00:57,  3.60s/it]Loading checkpoint shards:  50%|██████████████████████████████████████████████████████████████████████████████▌                                                                              | 15/30 [00:52<00:52,  3.48s/it]Loading checkpoint shards:  53%|███████████████████████████████████████████████████████████████████████████████████▋                                                                         | 16/30 [00:56<00:46,  3.36s/it]Loading checkpoint shards:  57%|████████████████████████████████████████████████████████████████████████████████████████▉                                                                    | 17/30 [01:01<00:52,  4.08s/it]Loading checkpoint shards:  60%|██████████████████████████████████████████████████████████████████████████████████████████████▏                                                              | 18/30 [01:05<00:46,  3.91s/it]Loading checkpoint shards:  63%|███████████████████████████████████████████████████████████████████████████████████████████████████▍                                                         | 19/30 [01:08<00:40,  3.72s/it]Loading checkpoint shards:  67%|████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                    | 20/30 [01:11<00:35,  3.51s/it]Loading checkpoint shards:  70%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                               | 21/30 [01:14<00:31,  3.47s/it]Loading checkpoint shards:  73%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                         | 22/30 [01:18<00:27,  3.38s/it]Loading checkpoint shards:  77%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                    | 23/30 [01:21<00:24,  3.45s/it]Loading checkpoint shards:  80%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                               | 24/30 [01:25<00:20,  3.46s/it]Loading checkpoint shards:  83%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                          | 25/30 [01:28<00:17,  3.42s/it]Loading checkpoint shards:  87%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                     | 26/30 [01:31<00:13,  3.31s/it]Loading checkpoint shards:  90%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎               | 27/30 [01:35<00:10,  3.37s/it]Loading checkpoint shards:  93%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌          | 28/30 [01:38<00:06,  3.36s/it]Loading checkpoint shards:  97%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊     | 29/30 [01:41<00:03,  3.33s/it]Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [01:43<00:00,  2.74s/it]Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [01:43<00:00,  3.44s/it]
Sanitized code outputs will be saved to evalplus_results/humaneval/meta-llama--Llama-3.1-70B-Instruct_hf_temp_0.0_gamma_5_610550.jsonl
Raw outputs will be saved to evalplus_results/humaneval/meta-llama--Llama-3.1-70B-Instruct_hf_temp_0.0_gamma_5_610550.raw.jsonl
Codegen: HumanEval/0 @ meta-llama/Llama-3.1-70B-Instruct
/tmp/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/tmp/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
627
430
Total tokens generated so far 430
Codegen: HumanEval/1 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
615
427
Total tokens generated so far 857
Codegen: HumanEval/2 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
510
352
Total tokens generated so far 1209
Codegen: HumanEval/3 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
546
354
Total tokens generated so far 1563
Codegen: HumanEval/4 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
533
343
Total tokens generated so far 1906
Codegen: HumanEval/5 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
548
383
Total tokens generated so far 2289
Codegen: HumanEval/6 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
542
357
Total tokens generated so far 2646
Codegen: HumanEval/7 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
475
309
Total tokens generated so far 2955
Codegen: HumanEval/8 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
566
383
Total tokens generated so far 3338
Codegen: HumanEval/9 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
619
446
Total tokens generated so far 3784
Codegen: HumanEval/10 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
588
373
Total tokens generated so far 4157
Codegen: HumanEval/11 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
519
370
Total tokens generated so far 4527
Codegen: HumanEval/12 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
390
215
Total tokens generated so far 4742
Codegen: HumanEval/13 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
440
297
Total tokens generated so far 5039
Codegen: HumanEval/14 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
347
215
Total tokens generated so far 5254
Codegen: HumanEval/15 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
356
215
Total tokens generated so far 5469
Codegen: HumanEval/16 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
404
261
Total tokens generated so far 5730
Codegen: HumanEval/17 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
699
459
Total tokens generated so far 6189
Codegen: HumanEval/18 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
560
399
Total tokens generated so far 6588
Codegen: HumanEval/19 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
586
408
Total tokens generated so far 6996
Codegen: HumanEval/20 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
674
435
Total tokens generated so far 7431
Codegen: HumanEval/21 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
762
566
Total tokens generated so far 7997
Codegen: HumanEval/22 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
407
243
Total tokens generated so far 8240
Codegen: HumanEval/23 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
282
166
Total tokens generated so far 8406
Codegen: HumanEval/24 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
548
423
Total tokens generated so far 8829
Codegen: HumanEval/25 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
581
382
Total tokens generated so far 9211
Codegen: HumanEval/26 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
511
354
Total tokens generated so far 9565
Codegen: HumanEval/27 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
316
195
Total tokens generated so far 9760
Codegen: HumanEval/28 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
460
330
Total tokens generated so far 10090
Codegen: HumanEval/29 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
451
291
Total tokens generated so far 10381
Codegen: HumanEval/30 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
416
230
Total tokens generated so far 10611
Codegen: HumanEval/31 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
490
318
Total tokens generated so far 10929
Codegen: HumanEval/32 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
765
433
Total tokens generated so far 11362
Codegen: HumanEval/33 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
646
423
Total tokens generated so far 11785
Codegen: HumanEval/34 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
457
311
Total tokens generated so far 12096
Codegen: HumanEval/35 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
463
308
Total tokens generated so far 12404
Codegen: HumanEval/36 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
439
287
Total tokens generated so far 12691
Codegen: HumanEval/37 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
541
345
Total tokens generated so far 13036
Codegen: HumanEval/38 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
1255
1024
Total tokens generated so far 14060
Codegen: HumanEval/39 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
537
367
Total tokens generated so far 14427
Codegen: HumanEval/40 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
686
454
Total tokens generated so far 14881
Codegen: HumanEval/41 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
453
229
Total tokens generated so far 15110
Codegen: HumanEval/42 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
495
314
Total tokens generated so far 15424
Codegen: HumanEval/43 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
498
268
Total tokens generated so far 15692
Codegen: HumanEval/44 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
381
217
Total tokens generated so far 15909
Codegen: HumanEval/45 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
331
212
Total tokens generated so far 16121
Codegen: HumanEval/46 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
673
429
Total tokens generated so far 16550
Codegen: HumanEval/47 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
569
421
Total tokens generated so far 16971
Codegen: HumanEval/48 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
391
247
Total tokens generated so far 17218
Codegen: HumanEval/49 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
544
366
Total tokens generated so far 17584
Codegen: HumanEval/50 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
564
401
Total tokens generated so far 17985
Codegen: HumanEval/51 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
474
279
Total tokens generated so far 18264
Codegen: HumanEval/52 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
393
241
Total tokens generated so far 18505
Codegen: HumanEval/53 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
398
272
Total tokens generated so far 18777
Codegen: HumanEval/54 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
520
307
Total tokens generated so far 19084
Codegen: HumanEval/55 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
441
316
Total tokens generated so far 19400
Codegen: HumanEval/56 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
478
313
Total tokens generated so far 19713
Codegen: HumanEval/57 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
508
348
Total tokens generated so far 20061
Codegen: HumanEval/58 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
579
392
Total tokens generated so far 20453
Codegen: HumanEval/59 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
810
674
Total tokens generated so far 21127
Codegen: HumanEval/60 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
395
228
Total tokens generated so far 21355
Codegen: HumanEval/61 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
470
308
Total tokens generated so far 21663
Codegen: HumanEval/62 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
493
312
Total tokens generated so far 21975
Codegen: HumanEval/63 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
641
419
Total tokens generated so far 22394
Codegen: HumanEval/64 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
678
479
Total tokens generated so far 22873
Codegen: HumanEval/65 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
522
367
Total tokens generated so far 23240
Codegen: HumanEval/66 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
398
213
Total tokens generated so far 23453
Codegen: HumanEval/67 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
649
366
Total tokens generated so far 23819
Codegen: HumanEval/68 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
818
433
Total tokens generated so far 24252
Codegen: HumanEval/69 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
701
463
Total tokens generated so far 24715
Codegen: HumanEval/70 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
584
387
Total tokens generated so far 25102
Codegen: HumanEval/71 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
596
402
Total tokens generated so far 25504
Codegen: HumanEval/72 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
685
382
Total tokens generated so far 25886
Codegen: HumanEval/73 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
605
375
Total tokens generated so far 26261
Codegen: HumanEval/74 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
700
434
Total tokens generated so far 26695
Codegen: HumanEval/75 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
860
708
Total tokens generated so far 27403
Codegen: HumanEval/76 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
537
322
Total tokens generated so far 27725
Codegen: HumanEval/77 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
427
244
Total tokens generated so far 27969
Codegen: HumanEval/78 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
820
448
Total tokens generated so far 28417
Codegen: HumanEval/79 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
428
216
Total tokens generated so far 28633
Codegen: HumanEval/80 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
564
370
Total tokens generated so far 29003
Codegen: HumanEval/81 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
842
499
Total tokens generated so far 29502
Codegen: HumanEval/82 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
368
214
Total tokens generated so far 29716
Codegen: HumanEval/83 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
480
363
Total tokens generated so far 30079
Codegen: HumanEval/84 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
499
293
Total tokens generated so far 30372
Codegen: HumanEval/85 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
419
291
Total tokens generated so far 30663
Codegen: HumanEval/86 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
422
219
Total tokens generated so far 30882
Codegen: HumanEval/87 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
858
526
Total tokens generated so far 31408
Codegen: HumanEval/88 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
668
392
Total tokens generated so far 31800
Codegen: HumanEval/89 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
541
358
Total tokens generated so far 32158
Codegen: HumanEval/90 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
484
287
Total tokens generated so far 32445
Codegen: HumanEval/91 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
499
319
Total tokens generated so far 32764
Codegen: HumanEval/92 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
545
337
Total tokens generated so far 33101
Codegen: HumanEval/93 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
548
366
Total tokens generated so far 33467
Codegen: HumanEval/94 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
878
519
Total tokens generated so far 33986
Codegen: HumanEval/95 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
671
429
Total tokens generated so far 34415
Codegen: HumanEval/96 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
566
348
Total tokens generated so far 34763
Codegen: HumanEval/97 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
450
279
Total tokens generated so far 35042
Codegen: HumanEval/98 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
426
281
Total tokens generated so far 35323
Codegen: HumanEval/99 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
604
375
Total tokens generated so far 35698
Codegen: HumanEval/100 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
625
420
Total tokens generated so far 36118
Codegen: HumanEval/101 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
482
293
Total tokens generated so far 36411
Codegen: HumanEval/102 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
858
691
Total tokens generated so far 37102
Codegen: HumanEval/103 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
573
360
Total tokens generated so far 37462
Codegen: HumanEval/104 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
450
275
Total tokens generated so far 37737
Codegen: HumanEval/105 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
775
426
Total tokens generated so far 38163
Codegen: HumanEval/106 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
585
388
Total tokens generated so far 38551
Codegen: HumanEval/107 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
640
356
Total tokens generated so far 38907
Codegen: HumanEval/108 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
535
338
Total tokens generated so far 39245
Codegen: HumanEval/109 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
691
319
Total tokens generated so far 39564
Codegen: HumanEval/110 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
768
498
Total tokens generated so far 40062
Codegen: HumanEval/111 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
762
538
Total tokens generated so far 40600
Codegen: HumanEval/112 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
574
339
Total tokens generated so far 40939
Codegen: HumanEval/113 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
666
420
Total tokens generated so far 41359
Codegen: HumanEval/114 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
486
325
Total tokens generated so far 41684
Codegen: HumanEval/115 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
738
343
Total tokens generated so far 42027
Codegen: HumanEval/116 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
551
308
Total tokens generated so far 42335
Codegen: HumanEval/117 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
723
469
Total tokens generated so far 42804
Codegen: HumanEval/118 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
562
340
Total tokens generated so far 43144
Codegen: HumanEval/119 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
573
332
Total tokens generated so far 43476
Codegen: HumanEval/120 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
623
333
Total tokens generated so far 43809
Codegen: HumanEval/121 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
481
311
Total tokens generated so far 44120
Codegen: HumanEval/122 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
608
407
Total tokens generated so far 44527
Codegen: HumanEval/123 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
1328
1024
Total tokens generated so far 45551
Codegen: HumanEval/124 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
1017
681
Total tokens generated so far 46232
Codegen: HumanEval/125 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
601
388
Total tokens generated so far 46620
Codegen: HumanEval/126 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
779
457
Total tokens generated so far 47077
Codegen: HumanEval/127 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
727
400
Total tokens generated so far 47477
Codegen: HumanEval/128 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
492
297
Total tokens generated so far 47774
Codegen: HumanEval/129 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
1251
782
Total tokens generated so far 48556
Codegen: HumanEval/130 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
898
590
Total tokens generated so far 49146
Codegen: HumanEval/131 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
441
297
Total tokens generated so far 49443
Codegen: HumanEval/132 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
527
321
Total tokens generated so far 49764
Codegen: HumanEval/133 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
483
253
Total tokens generated so far 50017
Codegen: HumanEval/134 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
531
321
Total tokens generated so far 50338
Codegen: HumanEval/135 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
485
309
Total tokens generated so far 50647
Codegen: HumanEval/136 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
640
429
Total tokens generated so far 51076
Codegen: HumanEval/137 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
623
409
Total tokens generated so far 51485
Codegen: HumanEval/138 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
560
410
Total tokens generated so far 51895
Codegen: HumanEval/139 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
527
359
Total tokens generated so far 52254
Codegen: HumanEval/140 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
632
453
Total tokens generated so far 52707
Codegen: HumanEval/141 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
904
617
Total tokens generated so far 53324
Codegen: HumanEval/142 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
543
301
Total tokens generated so far 53625
Codegen: HumanEval/143 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
506
286
Total tokens generated so far 53911
Codegen: HumanEval/144 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
585
357
Total tokens generated so far 54268
Codegen: HumanEval/145 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
620
432
Total tokens generated so far 54700
Codegen: HumanEval/146 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
600
408
Total tokens generated so far 55108
Codegen: HumanEval/147 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
580
342
Total tokens generated so far 55450
Codegen: HumanEval/148 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
711
427
Total tokens generated so far 55877
Codegen: HumanEval/149 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
568
308
Total tokens generated so far 56185
Codegen: HumanEval/150 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
506
347
Total tokens generated so far 56532
Codegen: HumanEval/151 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
479
269
Total tokens generated so far 56801
Codegen: HumanEval/152 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
630
327
Total tokens generated so far 57128
Codegen: HumanEval/153 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
752
402
Total tokens generated so far 57530
Codegen: HumanEval/154 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
598
396
Total tokens generated so far 57926
Codegen: HumanEval/155 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
527
389
Total tokens generated so far 58315
Codegen: HumanEval/156 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
587
413
Total tokens generated so far 58728
Codegen: HumanEval/157 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
527
346
Total tokens generated so far 59074
Codegen: HumanEval/158 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
573
380
Total tokens generated so far 59454
Codegen: HumanEval/159 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
760
410
Total tokens generated so far 59864
Codegen: HumanEval/160 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
827
547
Total tokens generated so far 60411
Codegen: HumanEval/161 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
517
337
Total tokens generated so far 60748
Codegen: HumanEval/162 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
406
257
Total tokens generated so far 61005
Codegen: HumanEval/163 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
518
341
Total tokens generated so far 61346
HuggingFaceDecoder •100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 164/164 • 3:22:09
Execution time exactly: 12129.31 seconds
Load from ground-truth from /tmp/.cache/evalplus/fe585eb4df8c88d844eeb463ea4d0302.pkl
Reading samples...
0it [00:00, ?it/s]1it [00:00,  1.41it/s]164it [00:00, 223.69it/s]
  0%|                                                                                                                                                                                                | 0/164 [00:00<?, ?it/s]  1%|█                                                                                                                                                                                       | 1/164 [00:00<01:29,  1.83it/s]  5%|████████▉                                                                                                                                                                               | 8/164 [00:00<00:11, 13.49it/s]  7%|████████████▎                                                                                                                                                                          | 11/164 [00:01<00:15,  9.77it/s] 10%|█████████████████▊                                                                                                                                                                     | 16/164 [00:01<00:14, 10.49it/s] 11%|████████████████████                                                                                                                                                                   | 18/164 [00:01<00:13, 10.92it/s] 15%|██████████████████████████▊                                                                                                                                                            | 24/164 [00:02<00:11, 12.11it/s] 16%|█████████████████████████████                                                                                                                                                          | 26/164 [00:02<00:11, 11.67it/s] 18%|█████████████████████████████████▍                                                                                                                                                     | 30/164 [00:02<00:11, 11.58it/s] 20%|███████████████████████████████████▋                                                                                                                                                   | 32/164 [00:02<00:12, 10.95it/s] 22%|████████████████████████████████████████▏                                                                                                                                              | 36/164 [00:03<00:12, 10.52it/s] 23%|██████████████████████████████████████████▍                                                                                                                                            | 38/164 [00:03<00:11, 11.34it/s] 24%|████████████████████████████████████████████▋                                                                                                                                          | 40/164 [00:03<00:10, 12.28it/s] 26%|██████████████████████████████████████████████▊                                                                                                                                        | 42/164 [00:03<00:12,  9.71it/s] 27%|██████████████████████████████████████████████████▏                                                                                                                                    | 45/164 [00:04<00:09, 12.01it/s] 29%|████████████████████████████████████████████████████▍                                                                                                                                  | 47/164 [00:04<00:08, 13.08it/s] 30%|██████████████████████████████████████████████████████▋                                                                                                                                | 49/164 [00:04<00:11,  9.81it/s] 33%|████████████████████████████████████████████████████████████▎                                                                                                                          | 54/164 [00:04<00:07, 13.80it/s] 34%|██████████████████████████████████████████████████████████████▍                                                                                                                        | 56/164 [00:04<00:07, 14.13it/s] 35%|████████████████████████████████████████████████████████████████▋                                                                                                                      | 58/164 [00:05<00:09, 11.37it/s] 37%|████████████████████████████████████████████████████████████████████                                                                                                                   | 61/164 [00:05<00:07, 13.05it/s] 38%|██████████████████████████████████████████████████████████████████████▎                                                                                                                | 63/164 [00:05<00:08, 12.41it/s] 40%|████████████████████████████████████████████████████████████████████████▌                                                                                                              | 65/164 [00:05<00:08, 11.34it/s] 42%|████████████████████████████████████████████████████████████████████████████▉                                                                                                          | 69/164 [00:05<00:06, 14.81it/s] 43%|███████████████████████████████████████████████████████████████████████████████▏                                                                                                       | 71/164 [00:06<00:07, 13.12it/s] 45%|█████████████████████████████████████████████████████████████████████████████████▍                                                                                                     | 73/164 [00:06<00:07, 12.42it/s] 47%|█████████████████████████████████████████████████████████████████████████████████████▉                                                                                                 | 77/164 [00:06<00:05, 15.43it/s] 48%|████████████████████████████████████████████████████████████████████████████████████████▏                                                                                              | 79/164 [00:06<00:06, 13.04it/s] 49%|██████████████████████████████████████████████████████████████████████████████████████████▍                                                                                            | 81/164 [00:06<00:06, 12.57it/s] 52%|██████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                        | 85/164 [00:07<00:05, 15.77it/s] 53%|█████████████████████████████████████████████████████████████████████████████████████████████████                                                                                      | 87/164 [00:07<00:05, 13.14it/s] 54%|███████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                                   | 89/164 [00:07<00:06, 12.49it/s] 57%|███████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                               | 93/164 [00:07<00:04, 15.38it/s] 58%|██████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                             | 95/164 [00:07<00:05, 12.24it/s] 59%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                          | 97/164 [00:08<00:05, 12.34it/s] 61%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                       | 100/164 [00:08<00:04, 13.42it/s] 62%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                    | 102/164 [00:08<00:06,  9.56it/s] 64%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                 | 105/164 [00:08<00:05, 10.82it/s] 65%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                               | 107/164 [00:09<00:06,  8.63it/s] 68%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                          | 111/164 [00:09<00:04, 11.52it/s] 69%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                        | 113/164 [00:09<00:04, 11.12it/s] 70%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                      | 115/164 [00:09<00:04, 10.28it/s] 72%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                   | 118/164 [00:10<00:03, 12.33it/s] 73%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                | 120/164 [00:10<00:03, 11.36it/s] 74%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                              | 122/164 [00:10<00:06,  6.71it/s] 79%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                      | 129/164 [00:11<00:02, 13.30it/s] 80%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                   | 132/164 [00:11<00:03,  9.24it/s] 84%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                            | 138/164 [00:12<00:02, 10.93it/s] 85%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                          | 140/164 [00:12<00:02, 10.18it/s] 88%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                     | 145/164 [00:12<00:01,  9.86it/s] 90%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                 | 148/164 [00:12<00:01, 11.64it/s] 92%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌              | 151/164 [00:13<00:01,  9.85it/s] 93%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊            | 153/164 [00:13<00:01, 10.41it/s] 96%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏       | 157/164 [00:13<00:00, 10.27it/s] 97%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍     | 159/164 [00:14<00:00, 10.58it/s] 99%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉ | 163/164 [00:15<00:00,  6.88it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 164/164 [00:16<00:00,  4.07it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 164/164 [00:16<00:00, 10.22it/s]
humaneval (base tests)
pass@1:	0.774
humaneval+ (base + extra tests)
pass@1:	0.713
Execution time for HF, run=2: 12267s
Running big command HF, run=3
Greedy decoding ON (--greedy): setting bs=1, n_samples=1, temperature=0
Initializing a decoder model: meta-llama/Llama-3.1-70B-Instruct ...
kwargs = {'batch_size': 1, 'temperature': 0.0, 'instruction_prefix': 'Please provide a self-contained Python script that solves the following problem in a markdown code block:', 'response_prefix': 'Below is a Python script with a self-contained function that solves the problem and passes corresponding tests:', 'trust_remote_code': 'true', 'dtype': 'bfloat16'}
self.eos = ['<|endoftext|>', '<|endofmask|>', '</s>', '\nif __name__', '\ndef main(', '\nprint(', '\n```\n']
/opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c104cuda9SetDeviceEi'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Loading checkpoint shards:   0%|                                                                                                                                                                      | 0/30 [00:00<?, ?it/s]Loading checkpoint shards:   3%|█████▎                                                                                                                                                        | 1/30 [00:03<01:32,  3.18s/it]Loading checkpoint shards:   7%|██████████▌                                                                                                                                                   | 2/30 [00:06<01:27,  3.13s/it]Loading checkpoint shards:  10%|███████████████▊                                                                                                                                              | 3/30 [00:09<01:30,  3.36s/it]Loading checkpoint shards:  13%|█████████████████████                                                                                                                                         | 4/30 [00:13<01:27,  3.37s/it]Loading checkpoint shards:  17%|██████████████████████████▎                                                                                                                                   | 5/30 [00:17<01:31,  3.65s/it]Loading checkpoint shards:  20%|███████████████████████████████▌                                                                                                                              | 6/30 [00:20<01:25,  3.58s/it]Loading checkpoint shards:  23%|████████████████████████████████████▊                                                                                                                         | 7/30 [00:24<01:20,  3.50s/it]Loading checkpoint shards:  27%|██████████████████████████████████████████▏                                                                                                                   | 8/30 [00:27<01:18,  3.57s/it]Loading checkpoint shards:  30%|███████████████████████████████████████████████▍                                                                                                              | 9/30 [00:31<01:16,  3.64s/it]Loading checkpoint shards:  33%|████████████████████████████████████████████████████▎                                                                                                        | 10/30 [00:37<01:25,  4.27s/it]Loading checkpoint shards:  37%|█████████████████████████████████████████████████████████▌                                                                                                   | 11/30 [00:43<01:29,  4.71s/it]Loading checkpoint shards:  40%|██████████████████████████████████████████████████████████████▊                                                                                              | 12/30 [00:47<01:22,  4.59s/it]Loading checkpoint shards:  43%|████████████████████████████████████████████████████████████████████                                                                                         | 13/30 [00:50<01:12,  4.26s/it]Loading checkpoint shards:  47%|█████████████████████████████████████████████████████████████████████████▎                                                                                   | 14/30 [00:54<01:05,  4.07s/it]Loading checkpoint shards:  50%|██████████████████████████████████████████████████████████████████████████████▌                                                                              | 15/30 [00:57<00:57,  3.82s/it]Loading checkpoint shards:  53%|███████████████████████████████████████████████████████████████████████████████████▋                                                                         | 16/30 [01:01<00:51,  3.68s/it]Loading checkpoint shards:  57%|████████████████████████████████████████████████████████████████████████████████████████▉                                                                    | 17/30 [01:04<00:46,  3.60s/it]Loading checkpoint shards:  60%|██████████████████████████████████████████████████████████████████████████████████████████████▏                                                              | 18/30 [01:08<00:44,  3.70s/it]Loading checkpoint shards:  63%|███████████████████████████████████████████████████████████████████████████████████████████████████▍                                                         | 19/30 [01:12<00:40,  3.67s/it]Loading checkpoint shards:  67%|████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                    | 20/30 [01:15<00:36,  3.64s/it]Loading checkpoint shards:  70%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                               | 21/30 [01:19<00:33,  3.70s/it]Loading checkpoint shards:  73%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                         | 22/30 [01:23<00:29,  3.64s/it]Loading checkpoint shards:  77%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                    | 23/30 [01:27<00:26,  3.76s/it]Loading checkpoint shards:  80%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                               | 24/30 [01:30<00:22,  3.76s/it]Loading checkpoint shards:  83%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                          | 25/30 [01:36<00:21,  4.35s/it]Loading checkpoint shards:  87%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                     | 26/30 [01:40<00:16,  4.13s/it]Loading checkpoint shards:  90%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎               | 27/30 [01:43<00:11,  3.97s/it]Loading checkpoint shards:  93%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌          | 28/30 [01:47<00:07,  3.82s/it]Loading checkpoint shards:  97%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊     | 29/30 [01:50<00:03,  3.74s/it]Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [01:52<00:00,  3.04s/it]Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [01:52<00:00,  3.74s/it]
Sanitized code outputs will be saved to evalplus_results/humaneval/meta-llama--Llama-3.1-70B-Instruct_hf_temp_0.0_gamma_5_482690.jsonl
Raw outputs will be saved to evalplus_results/humaneval/meta-llama--Llama-3.1-70B-Instruct_hf_temp_0.0_gamma_5_482690.raw.jsonl
Codegen: HumanEval/0 @ meta-llama/Llama-3.1-70B-Instruct
/tmp/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/tmp/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
627
430
Total tokens generated so far 430
Codegen: HumanEval/1 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
615
427
Total tokens generated so far 857
Codegen: HumanEval/2 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
510
352
Total tokens generated so far 1209
Codegen: HumanEval/3 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
546
354
Total tokens generated so far 1563
Codegen: HumanEval/4 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
533
343
Total tokens generated so far 1906
Codegen: HumanEval/5 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
548
383
Total tokens generated so far 2289
Codegen: HumanEval/6 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
542
357
Total tokens generated so far 2646
Codegen: HumanEval/7 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
475
309
Total tokens generated so far 2955
Codegen: HumanEval/8 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
566
383
Total tokens generated so far 3338
Codegen: HumanEval/9 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
619
446
Total tokens generated so far 3784
Codegen: HumanEval/10 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
588
373
Total tokens generated so far 4157
Codegen: HumanEval/11 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
519
370
Total tokens generated so far 4527
Codegen: HumanEval/12 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
390
215
Total tokens generated so far 4742
Codegen: HumanEval/13 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
440
297
Total tokens generated so far 5039
Codegen: HumanEval/14 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
347
215
Total tokens generated so far 5254
Codegen: HumanEval/15 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
356
215
Total tokens generated so far 5469
Codegen: HumanEval/16 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
404
261
Total tokens generated so far 5730
Codegen: HumanEval/17 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
699
459
Total tokens generated so far 6189
Codegen: HumanEval/18 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
560
399
Total tokens generated so far 6588
Codegen: HumanEval/19 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
586
408
Total tokens generated so far 6996
Codegen: HumanEval/20 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
674
435
Total tokens generated so far 7431
Codegen: HumanEval/21 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
762
566
Total tokens generated so far 7997
Codegen: HumanEval/22 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
407
243
Total tokens generated so far 8240
Codegen: HumanEval/23 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
282
166
Total tokens generated so far 8406
Codegen: HumanEval/24 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
548
423
Total tokens generated so far 8829
Codegen: HumanEval/25 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
581
382
Total tokens generated so far 9211
Codegen: HumanEval/26 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
511
354
Total tokens generated so far 9565
Codegen: HumanEval/27 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
316
195
Total tokens generated so far 9760
Codegen: HumanEval/28 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
460
330
Total tokens generated so far 10090
Codegen: HumanEval/29 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
451
291
Total tokens generated so far 10381
Codegen: HumanEval/30 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
416
230
Total tokens generated so far 10611
Codegen: HumanEval/31 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
490
318
Total tokens generated so far 10929
Codegen: HumanEval/32 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
765
433
Total tokens generated so far 11362
Codegen: HumanEval/33 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
646
423
Total tokens generated so far 11785
Codegen: HumanEval/34 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
457
311
Total tokens generated so far 12096
Codegen: HumanEval/35 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
463
308
Total tokens generated so far 12404
Codegen: HumanEval/36 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
439
287
Total tokens generated so far 12691
Codegen: HumanEval/37 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
541
345
Total tokens generated so far 13036
Codegen: HumanEval/38 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
1255
1024
Total tokens generated so far 14060
Codegen: HumanEval/39 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
537
367
Total tokens generated so far 14427
Codegen: HumanEval/40 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
686
454
Total tokens generated so far 14881
Codegen: HumanEval/41 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
453
229
Total tokens generated so far 15110
Codegen: HumanEval/42 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
495
314
Total tokens generated so far 15424
Codegen: HumanEval/43 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
498
268
Total tokens generated so far 15692
Codegen: HumanEval/44 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
381
217
Total tokens generated so far 15909
Codegen: HumanEval/45 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
331
212
Total tokens generated so far 16121
Codegen: HumanEval/46 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
673
429
Total tokens generated so far 16550
Codegen: HumanEval/47 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
569
421
Total tokens generated so far 16971
Codegen: HumanEval/48 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
391
247
Total tokens generated so far 17218
Codegen: HumanEval/49 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
544
366
Total tokens generated so far 17584
Codegen: HumanEval/50 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
564
401
Total tokens generated so far 17985
Codegen: HumanEval/51 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
474
279
Total tokens generated so far 18264
Codegen: HumanEval/52 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
393
241
Total tokens generated so far 18505
Codegen: HumanEval/53 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
398
272
Total tokens generated so far 18777
Codegen: HumanEval/54 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
520
307
Total tokens generated so far 19084
Codegen: HumanEval/55 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
441
316
Total tokens generated so far 19400
Codegen: HumanEval/56 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
478
313
Total tokens generated so far 19713
Codegen: HumanEval/57 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
508
348
Total tokens generated so far 20061
Codegen: HumanEval/58 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
579
392
Total tokens generated so far 20453
Codegen: HumanEval/59 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
810
674
Total tokens generated so far 21127
Codegen: HumanEval/60 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
395
228
Total tokens generated so far 21355
Codegen: HumanEval/61 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
470
308
Total tokens generated so far 21663
Codegen: HumanEval/62 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
493
312
Total tokens generated so far 21975
Codegen: HumanEval/63 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
641
419
Total tokens generated so far 22394
Codegen: HumanEval/64 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
678
479
Total tokens generated so far 22873
Codegen: HumanEval/65 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
522
367
Total tokens generated so far 23240
Codegen: HumanEval/66 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
398
213
Total tokens generated so far 23453
Codegen: HumanEval/67 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
649
366
Total tokens generated so far 23819
Codegen: HumanEval/68 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
818
433
Total tokens generated so far 24252
Codegen: HumanEval/69 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
701
463
Total tokens generated so far 24715
Codegen: HumanEval/70 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
584
387
Total tokens generated so far 25102
Codegen: HumanEval/71 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
596
402
Total tokens generated so far 25504
Codegen: HumanEval/72 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
685
382
Total tokens generated so far 25886
Codegen: HumanEval/73 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
605
375
Total tokens generated so far 26261
Codegen: HumanEval/74 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
700
434
Total tokens generated so far 26695
Codegen: HumanEval/75 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
860
708
Total tokens generated so far 27403
Codegen: HumanEval/76 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
537
322
Total tokens generated so far 27725
Codegen: HumanEval/77 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
427
244
Total tokens generated so far 27969
Codegen: HumanEval/78 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
820
448
Total tokens generated so far 28417
Codegen: HumanEval/79 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
428
216
Total tokens generated so far 28633
Codegen: HumanEval/80 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
564
370
Total tokens generated so far 29003
Codegen: HumanEval/81 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
842
499
Total tokens generated so far 29502
Codegen: HumanEval/82 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
368
214
Total tokens generated so far 29716
Codegen: HumanEval/83 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
480
363
Total tokens generated so far 30079
Codegen: HumanEval/84 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
499
293
Total tokens generated so far 30372
Codegen: HumanEval/85 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
419
291
Total tokens generated so far 30663
Codegen: HumanEval/86 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
422
219
Total tokens generated so far 30882
Codegen: HumanEval/87 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
858
526
Total tokens generated so far 31408
Codegen: HumanEval/88 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
668
392
Total tokens generated so far 31800
Codegen: HumanEval/89 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
541
358
Total tokens generated so far 32158
Codegen: HumanEval/90 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
484
287
Total tokens generated so far 32445
Codegen: HumanEval/91 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
499
319
Total tokens generated so far 32764
Codegen: HumanEval/92 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
545
337
Total tokens generated so far 33101
Codegen: HumanEval/93 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
548
366
Total tokens generated so far 33467
Codegen: HumanEval/94 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
878
519
Total tokens generated so far 33986
Codegen: HumanEval/95 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
671
429
Total tokens generated so far 34415
Codegen: HumanEval/96 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
566
348
Total tokens generated so far 34763
Codegen: HumanEval/97 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
450
279
Total tokens generated so far 35042
Codegen: HumanEval/98 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
426
281
Total tokens generated so far 35323
Codegen: HumanEval/99 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
604
375
Total tokens generated so far 35698
Codegen: HumanEval/100 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
625
420
Total tokens generated so far 36118
Codegen: HumanEval/101 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
482
293
Total tokens generated so far 36411
Codegen: HumanEval/102 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
858
691
Total tokens generated so far 37102
Codegen: HumanEval/103 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
573
360
Total tokens generated so far 37462
Codegen: HumanEval/104 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
450
275
Total tokens generated so far 37737
Codegen: HumanEval/105 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
775
426
Total tokens generated so far 38163
Codegen: HumanEval/106 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
585
388
Total tokens generated so far 38551
Codegen: HumanEval/107 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
640
356
Total tokens generated so far 38907
Codegen: HumanEval/108 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
535
338
Total tokens generated so far 39245
Codegen: HumanEval/109 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
691
319
Total tokens generated so far 39564
Codegen: HumanEval/110 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
768
498
Total tokens generated so far 40062
Codegen: HumanEval/111 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
762
538
Total tokens generated so far 40600
Codegen: HumanEval/112 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
574
339
Total tokens generated so far 40939
Codegen: HumanEval/113 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
666
420
Total tokens generated so far 41359
Codegen: HumanEval/114 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
486
325
Total tokens generated so far 41684
Codegen: HumanEval/115 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
738
343
Total tokens generated so far 42027
Codegen: HumanEval/116 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
551
308
Total tokens generated so far 42335
Codegen: HumanEval/117 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
723
469
Total tokens generated so far 42804
Codegen: HumanEval/118 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
562
340
Total tokens generated so far 43144
Codegen: HumanEval/119 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
573
332
Total tokens generated so far 43476
Codegen: HumanEval/120 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
623
333
Total tokens generated so far 43809
Codegen: HumanEval/121 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
481
311
Total tokens generated so far 44120
Codegen: HumanEval/122 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
608
407
Total tokens generated so far 44527
Codegen: HumanEval/123 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
1328
1024
Total tokens generated so far 45551
Codegen: HumanEval/124 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
1017
681
Total tokens generated so far 46232
Codegen: HumanEval/125 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
601
388
Total tokens generated so far 46620
Codegen: HumanEval/126 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
779
457
Total tokens generated so far 47077
Codegen: HumanEval/127 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
727
400
Total tokens generated so far 47477
Codegen: HumanEval/128 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
492
297
Total tokens generated so far 47774
Codegen: HumanEval/129 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
1251
782
Total tokens generated so far 48556
Codegen: HumanEval/130 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
898
590
Total tokens generated so far 49146
Codegen: HumanEval/131 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
441
297
Total tokens generated so far 49443
Codegen: HumanEval/132 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
527
321
Total tokens generated so far 49764
Codegen: HumanEval/133 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
483
253
Total tokens generated so far 50017
Codegen: HumanEval/134 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
531
321
Total tokens generated so far 50338
Codegen: HumanEval/135 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
485
309
Total tokens generated so far 50647
Codegen: HumanEval/136 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
640
429
Total tokens generated so far 51076
Codegen: HumanEval/137 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
623
409
Total tokens generated so far 51485
Codegen: HumanEval/138 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
560
410
Total tokens generated so far 51895
Codegen: HumanEval/139 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
527
359
Total tokens generated so far 52254
Codegen: HumanEval/140 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
632
453
Total tokens generated so far 52707
Codegen: HumanEval/141 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
904
617
Total tokens generated so far 53324
Codegen: HumanEval/142 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
543
301
Total tokens generated so far 53625
Codegen: HumanEval/143 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
506
286
Total tokens generated so far 53911
Codegen: HumanEval/144 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
585
357
Total tokens generated so far 54268
Codegen: HumanEval/145 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
620
432
Total tokens generated so far 54700
Codegen: HumanEval/146 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
600
408
Total tokens generated so far 55108
Codegen: HumanEval/147 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
580
342
Total tokens generated so far 55450
Codegen: HumanEval/148 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
711
427
Total tokens generated so far 55877
Codegen: HumanEval/149 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
568
308
Total tokens generated so far 56185
Codegen: HumanEval/150 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
506
347
Total tokens generated so far 56532
Codegen: HumanEval/151 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
479
269
Total tokens generated so far 56801
Codegen: HumanEval/152 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
630
327
Total tokens generated so far 57128
Codegen: HumanEval/153 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
752
402
Total tokens generated so far 57530
Codegen: HumanEval/154 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
598
396
Total tokens generated so far 57926
Codegen: HumanEval/155 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
527
389
Total tokens generated so far 58315
Codegen: HumanEval/156 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
587
413
Total tokens generated so far 58728
Codegen: HumanEval/157 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
527
346
Total tokens generated so far 59074
Codegen: HumanEval/158 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
573
380
Total tokens generated so far 59454
Codegen: HumanEval/159 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
760
410
Total tokens generated so far 59864
Codegen: HumanEval/160 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
827
547
Total tokens generated so far 60411
Codegen: HumanEval/161 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
517
337
Total tokens generated so far 60748
Codegen: HumanEval/162 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
406
257
Total tokens generated so far 61005
Codegen: HumanEval/163 @ meta-llama/Llama-3.1-70B-Instruct
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
518
341
Total tokens generated so far 61346
HuggingFaceDecoder •100% ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 164/164 • 3:22:16
Execution time exactly: 12136.54 seconds
Load from ground-truth from /tmp/.cache/evalplus/fe585eb4df8c88d844eeb463ea4d0302.pkl
Reading samples...
0it [00:00, ?it/s]1it [00:00,  1.39it/s]164it [00:00, 221.45it/s]
  0%|                                                                                                                                                                                                | 0/164 [00:00<?, ?it/s]  1%|█                                                                                                                                                                                       | 1/164 [00:00<01:36,  1.69it/s]  5%|██████████                                                                                                                                                                              | 9/164 [00:01<00:17,  8.63it/s] 10%|█████████████████▊                                                                                                                                                                     | 16/164 [00:01<00:12, 11.39it/s] 11%|████████████████████                                                                                                                                                                   | 18/164 [00:01<00:12, 11.81it/s] 14%|█████████████████████████▋                                                                                                                                                             | 23/164 [00:01<00:08, 16.43it/s] 16%|█████████████████████████████                                                                                                                                                          | 26/164 [00:02<00:12, 11.15it/s] 18%|█████████████████████████████████▍                                                                                                                                                     | 30/164 [00:02<00:12, 11.14it/s] 20%|███████████████████████████████████▋                                                                                                                                                   | 32/164 [00:03<00:12, 10.34it/s] 22%|████████████████████████████████████████▏                                                                                                                                              | 36/164 [00:03<00:11, 10.89it/s] 23%|██████████████████████████████████████████▍                                                                                                                                            | 38/164 [00:03<00:11, 10.79it/s] 24%|████████████████████████████████████████████▋                                                                                                                                          | 40/164 [00:03<00:10, 11.92it/s] 26%|██████████████████████████████████████████████▊                                                                                                                                        | 42/164 [00:03<00:12, 10.05it/s] 27%|██████████████████████████████████████████████████▏                                                                                                                                    | 45/164 [00:04<00:09, 12.24it/s] 29%|████████████████████████████████████████████████████▍                                                                                                                                  | 47/164 [00:04<00:09, 12.80it/s] 30%|██████████████████████████████████████████████████████▋                                                                                                                                | 49/164 [00:04<00:11, 10.12it/s] 32%|███████████████████████████████████████████████████████████▏                                                                                                                           | 53/164 [00:04<00:07, 14.38it/s] 34%|█████████████████████████████████████████████████████████████▎                                                                                                                         | 55/164 [00:04<00:07, 14.66it/s] 35%|███████████████████████████████████████████████████████████████▌                                                                                                                       | 57/164 [00:05<00:09, 10.85it/s] 37%|████████████████████████████████████████████████████████████████████                                                                                                                   | 61/164 [00:05<00:08, 11.78it/s] 39%|███████████████████████████████████████████████████████████████████████▍                                                                                                               | 64/164 [00:05<00:08, 11.79it/s] 40%|█████████████████████████████████████████████████████████████████████████▋                                                                                                             | 66/164 [00:05<00:08, 12.21it/s] 42%|████████████████████████████████████████████████████████████████████████████▉                                                                                                          | 69/164 [00:06<00:07, 12.45it/s] 44%|████████████████████████████████████████████████████████████████████████████████▎                                                                                                      | 72/164 [00:06<00:07, 12.54it/s] 45%|██████████████████████████████████████████████████████████████████████████████████▌                                                                                                    | 74/164 [00:06<00:07, 12.42it/s] 47%|█████████████████████████████████████████████████████████████████████████████████████▉                                                                                                 | 77/164 [00:06<00:06, 14.39it/s] 48%|████████████████████████████████████████████████████████████████████████████████████████▏                                                                                              | 79/164 [00:06<00:05, 15.10it/s] 49%|██████████████████████████████████████████████████████████████████████████████████████████▍                                                                                            | 81/164 [00:06<00:07, 11.54it/s] 52%|██████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                        | 85/164 [00:07<00:05, 15.54it/s] 53%|█████████████████████████████████████████████████████████████████████████████████████████████████                                                                                      | 87/164 [00:07<00:05, 13.96it/s] 54%|███████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                                   | 89/164 [00:07<00:06, 11.72it/s] 57%|███████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                               | 93/164 [00:07<00:04, 15.30it/s] 58%|██████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                             | 95/164 [00:07<00:04, 13.82it/s] 59%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                          | 97/164 [00:08<00:05, 11.37it/s] 61%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                       | 100/164 [00:08<00:05, 11.70it/s] 62%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                    | 102/164 [00:08<00:06,  9.19it/s] 64%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                 | 105/164 [00:08<00:05, 10.61it/s] 65%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                               | 107/164 [00:09<00:06,  8.66it/s] 68%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                          | 111/164 [00:09<00:04, 11.51it/s] 69%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                        | 113/164 [00:09<00:04, 11.30it/s] 70%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                      | 115/164 [00:09<00:04, 10.09it/s] 72%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                   | 118/164 [00:10<00:03, 12.98it/s] 73%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                | 120/164 [00:10<00:03, 12.06it/s] 74%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                              | 122/164 [00:10<00:06,  6.90it/s] 79%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                      | 129/164 [00:11<00:02, 13.83it/s] 80%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                   | 132/164 [00:11<00:03,  9.26it/s] 84%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                            | 138/164 [00:12<00:02, 11.15it/s] 85%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                          | 140/164 [00:12<00:02, 10.79it/s] 88%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                     | 145/164 [00:12<00:01, 10.52it/s] 90%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                  | 147/164 [00:12<00:01, 11.32it/s] 92%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌              | 151/164 [00:13<00:01, 10.46it/s] 93%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊            | 153/164 [00:13<00:00, 11.41it/s] 96%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏       | 157/164 [00:13<00:00, 10.61it/s] 97%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍     | 159/164 [00:13<00:00, 11.58it/s] 99%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉ | 163/164 [00:14<00:00,  7.29it/s]100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 164/164 [00:16<00:00, 10.21it/s]
humaneval (base tests)
pass@1:	0.774
humaneval+ (base + extra tests)
pass@1:	0.713
Execution time for HF, run=3: 12282s
